---
output: 
  pdf_document:
    citation_package: natbib
    fig_caption: true
    extra_dependencies: "subfig"
    #template: sv.latex.ms.tex
    
    
title: "Linear Regression Modelling of Mortgage Yield"
subtitle: "Cruz, Fennessy, Grobbelaar, Volpatti"
date: "`r format(Sys.time(), '%B %d, %Y')`"
bibliography: biblio.bib
biblio-style: apa
geometry: margin=2.6cm
fontsize: 12pt 
header-includes:
  - \usepackage{fancyhdr}
  - \pagestyle{fancy}
  - \fancyhf{}
  - \lhead{Cruz, Fennessy, Grobbelaar, Volpatti}
  #- \rhead{\thepage}
  - \setlength{\headheight}{15pt}                         # Adjust header height if needed
  - \usepackage{geometry}
  - \geometry{a4paper}
  - \usepackage[numbers]{natbib}
  - \usepackage{graphicx}
  - \usepackage[T1]{fontenc}
  - \usepackage[utf8]{inputenc}
  - \usepackage{textcomp}
  - \usepackage{gensymb}
  - \usepackage{amsmath}
  - \usepackage{amssymb}
  - \usepackage[running]{lineno}
  - \usepackage{setspace}
  - \usepackage{float}
  - \floatplacement{figure}{H}
  - \setlength{\abovecaptionskip}{-3pt}
  - \setlength{\belowcaptionskip}{-4pt}
  - \setlength{\textfloatsep}{-7pt}
  - \usepackage{caption}
  - \captionsetup[figure]{skip=0pt}
  - \usepackage{titlesec}
  - \titlespacing*{\section}{0pt}{10pt}{1pt}
  - \titlespacing*{\subsection}{0pt}{10pt}{1pt}
  - \titlespacing*{\subsubsection}{0pt}{10pt}{1pt}
always_allow_html: true
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

```{r, include= FALSE, warning=FALSE}
#citations:
#have a bib file and then to cite is [@nameofbib]
library(readr)
library(ggplot2)
library(cowplot)
library(tidyr)
library(MASS)  # For stepwise model selection
library(car)  # For VIF calculation
library(stargazer)  # For model summary table
library(corrplot)
library(xtable)  # Load the package
library(dplyr)
library(knitr)
library(kableExtra)
library(corrplot)
library(ggplot2)
library(pastecs)
library(psych)
library(lmtest)
library(graphics)
library(gridExtra)  # For arranging multiple plots
library(xtable)
library(gridExtra)
library(grid)
library(patchwork)
library(performance)
library(viridis)
library(wesanderson)
library(lmtest)
```

\vspace{-0.5cm} 

### Introduction

The role of a mortgage lender relies significantly on their mortgage yield, that is, the effective return earned from offering a mortgage for a house. Several factors play into forecasting this metric, not least the region that the house has been built in, and the financial profile of that area's residents. In this assignment, new home mortgage yields for 18 SMSAs (Standard Metropolitan Statistical Areas) are predicted from a series of financial and geographic features:

\begin{itemize}
    \item Average Loan/Mortgage Ratio  High values ==> Low Down Payments   (X1)
    \item Distance from Boston in miles  (X2)
    \item Savings per new unit built   (X3)
    \item Savings per capita   (X4)
    \item Pop inc 1950-1960 in %   (X5)
    \item Percent of first mortgage from inter-regional banks  (X6)
\end{itemize}

Initially, the features and correlations between them are investigated, offering insights into the subsequent linear regression model. The exploratory data analysis will offer insights into interpreting the performance of the final model, which aims to accurately predict the mortgage yield (%). 


### Exploratory Data Analysis

The Table 1 summarizes the quantitative measures of the selected factors chosen to model the mortgage yield for new houses. This data contains the average monthly yields from March 1963 to April 1964 for the 18 SMSAs covered in this analysis.

The authors divide the causal factors in three categories: (a) Import needs and transfer cost, (b) Risks and (c) Local market structures. 

Therefore, we will analyse this variables to reason the regression analysis that will follow. 

A. *Import needs and transfer costs*. The uneven distribution of held savings, the differential demand for mortgage funds, and the cost of transferring funds represent the three main components of this category. Column 4 shows that savings per capita vary significantly across SMSAs. However, as illustrated in Fig. 1(d) by the non-linear (flat) relationship and reflected in an r² value of 0.049, this variable explains little of the regional yield variations.

More importantly, 42% and 52% of regional yield differences are explained by variations in demand, represented by population growth and savings per unit built, respectively.

For transfer costs, distance from Boston is used as a measure, given that the city records the lowest yields, and previous studies have shown that mortgage costs rise as one moves south and west. Both the graphical representation in Fig. 1(b) and the r² value of 0.546 highlight the significance of this variable in explaining yield variance.

B. *Risks*. Among all factors, the loan-to-value ratio exhibited the strongest relationship with mortgage yields (r² = 0.654), confirming expectations that higher risk levels correlate with higher yields.

C. *Local Market Structure*. The degree of competition is said to influence financing charges, however the authors found that there was not significance for explaining the mortgage yields, hypothesizing that this is reflected in other variables.

Figure 1 illustrates the scatter plots, revealing a non-linear relationship between the independent variables and mortgage yield, with most exhibiting a log-like pattern. This characteristic was considered in the model development to ensure appropriate transformations and improve model performance.

### Univariate analysis
\vspace{0.2em}

```{r, echo=FALSE,warning=FALSE, table.pos="t",fig.pos="h"}
data_mort <- read.csv("http://users.stat.ufl.edu/~winner/data/myield.csv")

# Fix state names
data_mort$smsa <- ifelse(data_mort$smsa == "Los Angeles-Long Bea", "Los Angeles-Long Beach", data_mort$smsa)
data_mort$smsa <- ifelse(data_mort$smsa == "San Francisco-Oaklan", "San Francisco-Oakland", data_mort$smsa)

# Compute R² values for each variable in simple regressions
r2_values <- round(sapply(c("X1", "X2", "X3", "X4", "X5", "X6"), function(var) {
  model <- lm(mortYld ~ data_mort[[var]], data = data_mort)
  summary(model)$r.squared
}), 3)

# Compute Mean ± SD for each variable
mean_sd_values <- sapply(c("mortYld", "X1", "X2", "X3", "X4", "X5", "X6"), function(var) {
  mean_val <- mean(data_mort[[var]], na.rm = TRUE)
  sd_val <- sd(data_mort[[var]], na.rm = TRUE)
  sprintf("%.1f ± %.1f", mean_val, sd_val)  # Format as "Mean ± SD"
})

# Create rows for Mean ± SD and R² values
mean_sd_row <- c("Mean ± SD", "", mean_sd_values[-1])  # Skip smsa column
r2_row <- c("Coefficient of Determination (r²) with MortYld", "", as.character(r2_values))

# Create main table
table_data <- data_mort %>%
  select(smsa, mortYld, X1, X2, X3, X4, X5, X6) %>%
  rename(
    SMSA = smsa,
    `Mortgage Yield (%)` = mortYld,
    `Avg Loan/ Mortgage Ratio` = X1,
    `Distance from Boston (miles)` = X2,
    `Savings per New Unit Built` = X3,
    `Savings per Capita` = X4,
    `Population Increase 1950-1960 (%)` = X5,
    `% First Mortgage from Inter-regional Banks` = X6
  ) %>%
  mutate(across(everything(), as.character))  # Convert to character for new rows

# Convert rows into data frames
mean_sd_row_df <- as.data.frame(t(mean_sd_row), stringsAsFactors = FALSE)
r2_row_df <- as.data.frame(t(r2_row), stringsAsFactors = FALSE)

colnames(mean_sd_row_df) <- colnames(table_data)
colnames(r2_row_df) <- colnames(table_data)

# Bind Mean ± SD and R² row to the table
final_table <- bind_rows(table_data, mean_sd_row_df, r2_row_df)

# Print the final table using knitr::kable
knitr::kable(final_table, 
             caption = "Mortgage Yield and Explanatory Variables (Simple Regression Results and Summary Statistics)", 
             format = "latex", booktabs = TRUE, align = c("l", rep("c", 7))) %>%
  kable_styling(latex_options = c("scale_down", "hold_position")) %>%
  column_spec(1, width = "4cm", latex_valign = "m") %>%
  column_spec(2, width = "1.5cm", latex_valign = "m") %>%
  column_spec(3:8, width = "2.5cm", latex_valign = "m") %>%
  row_spec(nrow(final_table) - 1, font_size = 10) 

```

\vspace{-6em}
```{r scatter, echo=FALSE, warning=FALSE, fig.pos='H', fig.align='center', fig.margin = "0pt", fig.cap='Scatter plots with trend lines'}


data_mort <- read.csv("http://users.stat.ufl.edu/~winner/data/myield.csv")

# List to store ggplot objects
plot_list <- list()

# Create scatter plots with trend lines
for (var in c("X1", "X2", "X3", "X4", "X5", "X6")) {
  p_scat <- ggplot(data_mort, aes_string(x = var, y = "mortYld")) +
    geom_point(color = "blue") +  # Scatter points
    geom_smooth(method = "loess", color = "red", size = 1) +  # Trend line
    #labs(title = paste("Mortgage Yield vs", var), x = var, y = "mortYld") +
    theme_minimal()  +
    theme(plot.margin = margin(-0, 0, 0, 0))
  
  # Store plot
  plot_list[[var]] <- p_scat
}

# Arrange all plots in a 2x3 layout
#grid_scat <- grid.arrange(grobs = plot_list, ncol = 3, nrow = 2)

adjusted_plots <- lapply(plot_list, function(plot) {
  plot + theme(aspect.ratio = 1 / 2) +
    theme(plot.margin = margin(-0, 0, 0, 0))
})

# Combine plots in a grid with fixed sizes
final_patchwork <- wrap_plots(adjusted_plots, ncol = 3, nrow = 2) +
  plot_layout(guides = "collect") + # Collect legends if any
  theme(plot.margin = margin(-0, 0, 0, 0)) # Adjust margins if necessary

# Render the patchwork plot
final_patchwork

```


The histograms reveal that most variables exhibit varying degrees of right skewness. Thus, transformations of this variables may be necessary for statistical modeling to improve normality and interpretability, as discussed later.
\vspace{-3em}
```{r histograms, echo=FALSE, warning=FALSE, fig.cap = "Univariate graphical analysis - Histograms",fig.pos='H', ,out.height="80%",fig.align='center'}

data_mort <- read.csv("http://users.stat.ufl.edu/~winner/data/myield.csv")

# Create a list to store the plots
plot_list1 <- list()

for (var in c("X1", "X2", "X3", "X4", "X5", "X6")) {
  p_hist <- ggplot(data_mort, aes(x = .data[[var]]) ) +  # Correct column reference
    geom_histogram(aes(y = ..density..), color = "black", fill = "white", bins = 10) +
    labs(x = paste("Histogram of", var), y = "Density") +
    theme_minimal() +
    stat_function(
      fun = dnorm, 
      args = list(mean = mean(data_mort[[var]], na.rm = TRUE), 
                  sd = sd(data_mort[[var]], na.rm = TRUE)), 
      colour = "black", linewidth = 1
    )
  
  # Store plot in the list
  plot_list1[[var]] <- p_hist
}

# Arrange all plots in a 2x3 layout
#grid_hist <- grid.arrange(grobs = plot_list1, ncol = 3, nrow = 2, widths = unit(c(2, 2, 2), "null"),
  #heights = unit(c(1, 1), "null"))
adjusted_plots <- lapply(plot_list1, function(plot) {
  plot + theme(aspect.ratio = 1 / 2) # 1:1.5 height-to-width ratio
})

# Combine plots in a grid with fixed sizes
final_patchwork <- wrap_plots(adjusted_plots, ncol = 3, nrow = 2) +
  plot_layout(guides = "collect") + 
  theme(plot.margin = unit(c(0, 0, 0, 0), "cm")) # Adjust margins if necessary

# Render the patchwork plot
#final_patchwork

```


```{r histograms_adj, echo=FALSE, warning=FALSE, fig.cap = "Univariate graphical analysis - Histograms",fig.pos='H', ,fig.align='center', out.height="80%", out.width="80%"}

# Arrange the plots in a grid
adjusted_plots <- lapply(adjusted_plots, function(plot) {
    plot + theme(plot.margin = margin(-0, -0, -1, -0, "cm"))
})
grid_hist <- arrangeGrob(
  grobs = adjusted_plots,
  ncol = 3,
  nrow = 2
)

# Use a single grid.newpage to handle everything
grid.newpage()
pushViewport(viewport(layout = grid.layout(3, 1, heights = unit(c(0.05, 0.9, 0.05), "npc"))))

# Title
# grid.text("Univariate Graphical Analysis - Histograms",
#           gp = gpar(fontsize = 14, fontface = "bold"),
#           vp = viewport(layout.pos.row = 1))

# Main grid
grid.draw(grid_hist)

# # Caption
# grid.text("Data source: XYZ Dataset",
#           gp = gpar(fontsize = 10),
#           vp = viewport(layout.pos.row = 3))

popViewport()
```


\vspace{-2em}
### Correlation
The scatterplot matrix presents the partial correlations among the six explanatory variables (X1–X6), offering insights into their linear dependencies after controlling for other variables. The lower triangular panel displays bivariate scatterplots with fitted trend lines, illustrating potential nonlinear relationships. The upper triangular panel contains partial correlation coefficients, where values exceeding an absolute value of 0.4 are highlighted in red to denote strong associations. Notably, X3 and X4 exhibit the highest partial correlation (0.9), suggesting a strong linear relationship after adjusting for other variables. This analysis aids in assessing multicollinearity and refining variable selection for subsequent regression modeling.
\vspace{-3em}
```{r correlations, echo=FALSE, warning=FALSE, fig.cap= "correlations", fig.align='center',out.height="80%", out.width="80%"}
# Partial Correlation Function
p.cor <- function(x){
  inv <- solve(var(x))
  sdi <- diag(1/sqrt(diag(inv)))
  p.cor.mat <- -(sdi %*% inv %*% sdi)
  diag(p.cor.mat) <- 1
  rownames(p.cor.mat) <- colnames(p.cor.mat) <- colnames(x)
  return(p.cor.mat)
}

# Compute Partial Correlation Matrix
partial_cor_matrix <- p.cor(data_mort[, c("X1", "X2", "X3", "X4", "X5", "X6")])

# Find the column index that matches the list of values
find_column_index <- function(values_list, data_mort) {
  # Iterate over each column and compare the values
  for (i in 1:ncol(data_mort)) {
    if (all(data_mort[, i] == values_list)) {
      return(i)  # Return the column index if values match
    }
  }
  return(NULL)  # If no column matches, return NULL
}


# Custom panel for upper triangle (displaying partial correlation values)
panel_cor <- function(x, y, digits = 2, cex.cor = 1.5, ...) {
  usr <- par("usr"); on.exit(par(usr))  # Save plot settings

  par(usr = c(0, 1, 0, 1))  # Set plot region
  # Get variable names from x and y
  index_x = find_column_index(x, data_mort) - 2
  index_y = find_column_index(y, data_mort) - 2
  
  # Extract the partial correlation value from the precomputed matrix
  r <- partial_cor_matrix[index_x, index_y]
  
  # Format the correlation value
  txt <- format(c(r), digits = digits)[1]
  
  # Display the correlation value, with color based on strength
  text(0.5, 0.5, txt, cex = cex.cor, col = ifelse(abs(r) > 0.4, "red", "blue"))
}

# Scatterplot matrix with partial correlations
pairs(data_mort[, c("X1", "X2", "X3", "X4", "X5", "X6")], 
      lower.panel = panel.smooth,  # Scatterplot in lower
      upper.panel = panel_cor     # Correlation values in upper
      )
```



### Model Fitting
<!--
#- We fitted full linear model
#- Observation VIF, Correl(prev mentioned), R^2(prev mentioned) -> dropping X4 -->
Due to the high pairwise correlation that we observe for X3 and X4, we begin by initially considering the Variance Inflation factor (VIF) of a linear model that includes all predictors. VIF will be able to quantify how much inflation there is in our linear model due correlations between predictors 
```{r fig:VIF1, echo=FALSE, warning=FALSE, fig.align='center', fig.width=8, fig.height=4,results='hide',fig.cap=' ', out.height="90%", out.width="90%", fig.asp=0.5}
# Create Barplot
vif_values <- vif(lm(mortYld ~ X1 + X2 + X3 + X4 + X5 + X6, data = data_mort))
# Create Barplot
vif_data <- data.frame(Variable = names(vif_values), VIF = vif_values)
barplot_grob <- ggplot(vif_data, aes(x = reorder(Variable, -VIF), y = VIF, fill = Variable)) +
  geom_bar(stat = "identity", show.legend = FALSE, fill="skyblue") +
  coord_flip() +
  labs(title = "", x = "Variables", y = "VIF") +
  theme_minimal() + theme(
    plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), "cm"),  # Add margins to avoid clipping
  axis.text.x = element_text(angle = 45, hjust = 1),  # Rotate axis text for clarity
    axis.text.y = element_text(size = 7)  # Reduce text size if needed
  )


reduced_model <- lm(mortYld ~ X1 + X2 + X3 + X5 + X6, data = data_mort)

selected_reduced_model <- step(reduced_model, data = data_mort, direction = 'both')


# Create Table Using kableExtra
table_data <- as.data.frame(selected_reduced_model$anova)
colnames(table_data) <- c("Step", "Df", "Dev", "Resid.Df", "Resid.Dev", "AIC")  # Example column names
table_grob <- tableGrob(
  table_data,
  theme = ttheme_minimal(padding = unit(c(0.1, 0.1), "mm"))
)
# Round specific columns
table_data$Df <- round(table_data$Df, 2)
table_data$Dev <- round(table_data$Dev, 2)
table_data$Resid.Df <- round(table_data$Resid.Df, 2)
table_data$Resid.Dev <- round(table_data$Resid.Dev, 2)
table_data$AIC <- round(table_data$AIC, 2)


table_grob <- tableGrob(
  table_data,
  theme = ttheme_default(
    core = list(
      fg_params = list(col = "black", fontface = "plain"),
      bg_params = list(fill = "white", col = "white", lwd = 0)  # No shading and no column borders
    ),
    colhead = list(
      fg_params = list(col = "black", fontface = "bold", cex = 1),
      bg_params = list(fill = "white", col = "white", lwd = 1)  # Add lines only to header
    ),
    rowhead = list(
      fg_params = list(col = "black", fontface = "italic", cex = 1),
      bg_params = list(fill = "white", col = "white", lwd = 0)  # No shading for row headers
    ),
    padding = unit(c(2, 2), "mm")  # Adjust padding for spacing
  )
)
# Add sub-captions using textGrob for both barplot and table
barplot_subcap <- textGrob("Figure 4: Variance Inflation Factor (VIF)", gp = gpar(fontsize = 10, fontface = "plain"))
table_subcap <- textGrob("Table 2: Stepwise Model Selection Path", gp = gpar(fontsize = 10, fontface = "plain"))

# Arrange the Barplot and Table side by side with adjusted widths
par(mar = c(0, 0, 0, 0))
grid.arrange(
  ggplotGrob(barplot_grob), 
  table_grob, 
  barplot_subcap,
  table_subcap,
  ncol = 2, 
  widths = c(0.45, 0.45),  # Adjust relative width of barplot and table
  heights = unit(c(0.8, 0.1, 0.0, 0.0), "npc")  # Add space for captions at the bottom
)
# Arrange Barplot and Table Side by Side
#grid.arrange(ggplotGrob(barplot_grob), table_grob, ncol = 2)

```


```{r fig:VIF, echo=FALSE, warning=FALSE, fig.cap = "Full model VIF", fig.align='left', fig.pos='H', results='hide', fig.width=4}

# # Compute Variance Inflation Factor (VIF)
# vif_values <- vif(lm(mortYld ~ X1 + X2 + X3 + X4 + X5 + X6, data = data_mort))
# 
# barplot(vif_values, col = "skyblue")#, main = "Variance Inflation Factor (VIF)")
# 
# #cat("\nVIF Values:\n") 

```


We see from Figure 4 that the VIF values for X3 and X4 are above the standardly accepted value of 5, supporting the indication from the correlation matrix, implying high multicollinearity. In Table 1, we have the comparisons of adjusted $R^{2}$, and from that we see that X4 has a significantly lower $R^{2}$ than X3. Obersving the results from the correlation matrix, in conjunction with the high VIF values, and the large difference of adjusted $R^{2}$ for X3 and X4, we can conclude  X4 has low explanatory power in our initial model. Meaning that it is in with in a reasonable assumption that we can remove X4 from our model and not have it be detrimental to the model's explanatory power.

<!--- Run stepwise -> model will reduce down to X1, X2, X3 -->
The next step is to consider a linear model excluding X4:
$$
\text{mortYld}_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 X_{3i} + \beta_4 X_{5i} + \beta_5 X_{6i} \tag{1}
$$
After selecting our new model, we aim to have a balance of our model's explanatory power and its complexity. To do this we run a stepwise regression on the model. When implemented in R, the process will iteratively add and substract predictors. When considering candidate models in this way, the stepwise regression will aim to minimise AIC, which guides its choice of predictors for the final model. 
```{r reduced, echo=FALSE, warning=FALSE, fig.cap = "reduced model", fig.align='center',results='hide', fig.pos='h'}
# reduced_model <- lm(mortYld ~ X1 + X2 + X3 + X5 + X6, data = data_mort)
# 
# selected_reduced_model <- step(reduced_model, data = data_mort, direction = 'both')
# # Shows the path of the stepwise selection
# #selected_reduced_model$anova
```

```{r removed, echo=FALSE, warning=FALSE, table.pos='h',table.width=4,table.align='right'}
# kable(selected_reduced_model$anova, 
#       caption = "Stepwise model selection path", 
#       digits = 3, 
#       format = "latex",booktabs = TRUE) %>%
#   kable_styling(full_width = F, bootstrap_options = c("striped", "hover", "condensed", "responsive"), position = "center")

```
What we observe from Table 2, is that the stepwise regression process determined that removing X5 and X6 from our model, minimizes the AIC. This results in an optimized linear model of:
$$
\text{mortYld}_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 X_{3i} \tag{2}
$$
<!-- - Reiterate non linear observations -->
Now that we have our reduced model, we can focus in on the remaining predictors. As was shown previously in Figure 1, X1, X2, and X3 have seemingly non linear relationships with the mortgage yield. This would indicate that applying a non linear transform to our model, could potentially lead to improved fitting. We chose to consider three potential transforms, namely polynomial transform of the order 2:
$$
\text{mortYld}_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 X_{3i} + \beta_4 X_{1i}^2 + \beta_5 X_{2i}^2 + \beta_6 X_{3i}^2 \tag{3}
$$
A Log of the response(mortgage yield):
$$
\log(\text{mortYld}_i) = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_3 X_{3i} \tag{4}
$$
and lastly the log of the predictors:
$$
\text{mortYld}_i = \beta_0 + \beta_1 \log(X_{1i}) + \beta_2 \log(X_{2i}) + \beta_3 \log(X_{3i}) \tag{5}
$$
<!--- - RESET Test -> exlcude polynomial -->
Prior to model comparisons, we can perform a Ramsey RESET test. This will allow us to determine whether additional polynomial terms will add explanatory power. In R implementation, the RESET test will implement a standard F-test, to determine whether the additional polynomial terms have significant value. Our RESET test resulted in a $p=0.3$, indicating that an inclusion of polynomial terms offers no increased model fitting. This will allows to exclude the polynomial model in the further analysis.
<!--- - Conclude on best model --->
\\
Now that we have restricted our models to the initial reduced linear model (Eq. 2), the log of the response (Eq. 4), and the log of the predictors (Eq. 5), we evaluate the performance of these models against each other. We do this by looking at the AIC, BIC, RMSE, and Adjusted $R^2$. We can see the comparisons in Figure. 5, where we observe that the Log-Predictors model out performs the other two models in majority of the metrics. From the AIC and BIC, the lower values for Log-Predictors indicate that it optimizes model fitting and complexity better than the reduced and Log-Response. For RMSE, Log-Response out performs the other two, while for Adjusted $R^2$ Log-Predictors performs the best again. 
```{r , echo=FALSE, warning=FALSE, results='hide'}
reset_result <- resettest(selected_reduced_model, power = 2:3, type = "fitted")
#print(reset_result)
```


<!--- - Apply non linear transformations(LOG) --->
```{r nonlin,echo=FALSE, warning=FALSE, results='hide'}
#Implementing log model on outcome
log_model_outcome <- log(mortYld) ~ X1 + X2 + X3
log.model.lm <- lm(log_model_outcome, data = data_mort)
summary(log.model.lm)

#Log model on predictors
delta = 1e-16
log_model1 <- mortYld ~ log(X1) + log(X2+delta) + log(X3)
log.model.lm1 <- lm(log_model1, data = data_mort)
summary(log.model.lm1)
```

<!--- - Look at model comaprison metrics --->

```{r com_plots, echo=FALSE, warning=FALSE, fig.align='center',results='hide', fig.pos='h'}
# List of models
models <- list(
  "Reduced" = selected_reduced_model,
  "Log-Response" = log.model.lm,
  "Log-Predictors" = log.model.lm1
)

# Create performance comparison table
model_metrics <- compare_performance(models, metrics = c("AIC", "BIC", "R2", "Adj_R2", "RMSE"))
print(model_metrics)


# Melt metrics for plotting
plot_data <- model_metrics %>%
  pivot_longer(cols = c(AIC, BIC, RMSE), names_to = "Metric", values_to = "Value")

# Plot AIC values separately
p1 <- ggplot(plot_data[plot_data$Metric == "AIC",], aes(x = Name, y = Value, fill = Name)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(Value, 2)), vjust = -0.5, size = 2.5, color = "white") +
  labs(title = "AIC Values for All Models", y = "AIC Value", x = "") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_manual(values = wes_palette("Zissou1", n = 3,type='continuous')) + theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 8), 
    axis.text.y = element_text(size = 8),  
    axis.title.x = element_text(size = 10), 
    axis.title.y = element_text(size = 10),  
    plot.title = element_text(size = 12),
    legend.text = element_text(size = 8),
    legend.title = element_text(size = 10)
  )

# Plot BIC values separately
p2 <- ggplot(plot_data[plot_data$Metric == "BIC",], aes(x = Name, y = Value, fill = Name)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(Value, 2)), vjust = -0.5, size = 2.5, color = "white") +
  labs(title = "BIC Values for All Models", y = "BIC Value", x = "") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_manual(values = wes_palette("Zissou1", n = 3,type='continuous')) + theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 8), 
    axis.text.y = element_text(size = 8),  
    axis.title.x = element_text(size = 10), 
    axis.title.y = element_text(size = 10),  
    plot.title = element_text(size = 12),
    legend.text = element_text(size = 8),
    legend.title = element_text(size = 10)
  )

# Plot RMSE values separately
p3 <- ggplot(plot_data[plot_data$Metric == "RMSE",], aes(x = Name, y = Value, fill = Name)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(Value, 2)), vjust = 1.6, size = 2.5, color = "white") +
  labs(title = "RMSE Values for All Models", y = "RMSE", x = "") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_manual(values = wes_palette("Zissou1", n = 3,type='continuous')) + theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 8), 
    axis.text.y = element_text(size = 8),  
    axis.title.x = element_text(size = 10), 
    axis.title.y = element_text(size = 10),  
    plot.title = element_text(size = 12),
    legend.text = element_text(size = 8),
    legend.title = element_text(size = 10)
  )


# Calculate adjusted R² values
adj_r2 <- c(
  summary(selected_reduced_model)$adj.r.squared,
  summary(log.model.lm)$adj.r.squared,
  summary(log.model.lm1)$adj.r.squared
)

# Create data frame for adjusted R² values
adj_r2_df <- data.frame(
  Name = c("Reduced", "Log-Response", "Log-Predictors"),
  Adj_R2 = adj_r2
)

# Merge AIC and adjusted R² results
#comparison_df <- merge(aic_df, adj_r2_df, by = "Model", all = TRUE)

# Plot Adjusted R^2
#pal <- wes_palette("Zissou1", 3, type = "continuous")
p4 <- ggplot(adj_r2_df, aes(x = Name, y = Adj_R2, fill = Name)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(Adj_R2, 2)), vjust = 2, size = 2.5, color = "white") +
  labs(title = expression(paste("Adjusted ",R^2," for All Models")), y = expression(paste("Adjusted ",R^2)), x = "") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  #scale_fill_viridis(discrete = TRUE) + 
  scale_fill_manual(values = wes_palette("Zissou1", n = 3,type='continuous')) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 8), 
    axis.text.y = element_text(size = 8),  
    axis.title.x = element_text(size = 10), 
    axis.title.y = element_text(size = 10),  
    plot.title = element_text(size = 12),
    legend.text = element_text(size = 8),
    legend.title = element_text(size = 10)
  )



```
\vspace{-2cm}
```{r Model_metrics,echo=FALSE, warning=FALSE, fig.cap='Metrics for Model Comparisons',fig.align='center',out.height="80%", out.width="80%"}
(p1 + p2)/
(p3 + p4)
```

From these metrics, we can see that the Log-Predictors model is most suitable for modeling Mortgage Yield. Log-Response might perform best in prediction power (RMSE), but the magnitude of difference when compared to Mortgage Yield values is low, supporting then the decision to go with Log-Predictors.

<!--- - Final summary plots of best model --->
Now that we have selected the Log-Predictors as the best candidate model, we can evaluate it by looking at four diagnostic plots in Figure 6 (Residuals vs Fitted, Scale-Location, Q-Q Residuals, and Residuals vs Leverage). In the Residuals vs Fitted plot we want to look for a horizontal red line, indicating that as we add predictors we still remain with overall constant variance. In our model we can see some curvelinear trend, however, the scale of variation is relatively low around -0.2 to +0.1. 

```{r ,echo=FALSE, warning=FALSE, fig.align='center',results='hide', fig.cap='Diagnostic plots for the final model'}
# plot(compare_performance(selected_reduced_model,log.model.lm,log.model.lm1, rank = TRUE, verbose = FALSE))
# First extract the diagnostic plots
# Generate and modify plots in ONE STEP to prevent duplicates
# Generate and modify plots in ONE code chunk
# check_model(log.model.lm1, check = c("qq", "ncv", "vif", "outliers")) |>
#   plot() &  # Use & to apply themes globally
#   theme(
#     plot.title = element_text(hjust = 0.5, size = 9, margin = margin(b = 2)),
#     plot.subtitle = element_text(hjust = 0.5, size = 7),
#     plot.margin = margin(5, 5, 5, 5)  # Add spacing between subplots
#   )

```

```{r,echo=FALSE, warning=FALSE, fig.align='center',results='hide'}
x <- log.model.lm1  # Your model

mode_ <- function(model) {
  # Run check_model on the provided model with the specified checks.
  check_model(model)#, check = c("qq", "ncv", "outliers"))
  # Return the model invisibly.
  invisible(model)
}

# Now call the function on your model x
```

\vspace{-1cm}
```{r ,echo=FALSE, warning=FALSE, fig.align='center',results='hide',fig.cap='Diagnostic plots for the final model', out.height="80%", out.width="80%"}
# g <- ggplotGrob(plot(mode_(x)))
# g$layout
layout(matrix(1:4,ncol=2))
pp <- plot(mode_(x)) & theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 8), 
    axis.text.y = element_text(size = 1),  
    axis.title.x = element_text(size = 1), 
    axis.title.y = element_text(size = 10),  
    plot.title = element_text(size = 12),
    legend.text = element_text(size = 8),
    legend.title = element_text(size = 10)
  )

# &  # Use & to apply themes globally
#   theme(
#     plot.title = element_text(hjust = 0.5, size = 9, margin = margin(b = 2)),
#     plot.subtitle = element_text(hjust = 0.5, size = 7),
#     plot.margin = margin(5, 5, 5, 5)  # Add spacing between subplots
#   )
```

Scale-Location aims to again asses the spread variance of with predictors. The downward trend of the plot could indicate variance spread. From the Q-Q plot, we can see that the residuals follow the reference line. This is a positive sign for our model, and indicates that the assumption that residuals should follow a normal distribution is satisfied for our model. This implies that hypothesis tests with our model should have high reliability. The Residuals vs Leverage plot is to investigate how influential certain observations are. We can check this by noting if the points are falling within the Cooks distance contour lines. What we observe is that while we have a single observation outside, the remaining points are all within a reasonable range, indicating general model stability. Due to some of homoscedasticity we saw, we decided to run a Breusch-Pagan test (BP test) to validate whether it should be of concern. From the BP test we obtained values of  BP = 0.92096, and p-value = 0.8204, indicating that we do not have heteroscedasticity.
<!--- - BP test --->
```{r ,echo=FALSE, warning=FALSE, fig.align='center',results='hide'}
bp_test_result <- bptest(log.model.lm1)
print(bp_test_result)
```



### Discussion of any shortcomings of the final model

1. *Removal of X4 from consideration:* The introduction of X4 would have led to multicollinearity in the model, which itself leads to less meaningful and less accurate beta predictors. However, there is the possibility that X4 provided some additional information that X3 lacked; such an exclusion would lead to less predictive power and a less accurate model.

2. *The nature of the model training:* Standard practice when designing a model involves the testing of said model on a separate dataset. This allows us to conclude that the model is generalizable and is not simply overfit to the dataset used. As such, in the future, we would consider splitting the given data into testing and training sets so as to test and validate the model.

3. *Issues with the logarithmic model:* The logarithm of values less than or equal to zero is a forbidden operation. This excludes us from analysing data whose values are zero (e.g., houses in Boston, whose distance from Boston are zero). This resulted in us adding an infinitesimally small delta to allow us to perform this operation and consider these datapoints in our model without skewing the others much. In the future, we would consider what the best course of action would be between: 1. Adding the delta, 2. Removing Boston from consideration, 3. Modifying Boston’s data by inputting the average distance from Boston from all other cities.


### Conclusions

This report found that the log-transformed response model, using average loan-to-value ratio (X1), distance from Boston (X2), and savings per new unit built (X3), best explains mortgage yield variation across 18 SMSAs. This model was selected through stepwise regression and evaluated using AIC, BIC, RMSE, and adjusted R2, achieving an adjusted R2 of 0.80.
While statistically sound, the model has limitations—most notably, the exclusion of X4 due to multicollinearity, the absence of a test set for validation, and the need to adjust zero values for log transformation. These limitations suggest areas for refinement.
Despite this, the final model offers strong predictive utility and provides insight into the key economic and geographic factors affecting mortgage yields.